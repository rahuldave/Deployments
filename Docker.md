# Docker



Let's go through the steps for creating a Docker container for a Flask application using an SQLite database mounted from outside. We'll cover:

1. Setting up the Flask application.
2. Creating a Dockerfile with multi-stage builds.
3. Building and running the Docker container.
4. Accessing the SQLite database from the host filesystem.
5. Uploading the Docker image to a registry.

### Step 1: Setting Up the Flask Application

Create a simple Flask application. Here's a basic example:

**app.py**

```python
from flask import Flask, g, request
import sqlite3

app = Flask(__name__)
DATABASE = '/data/database.db'  # Path to the SQLite database file

def get_db():
    db = getattr(g, '_database', None)
    if db is None:
        db = g._database = sqlite3.connect(DATABASE)
    return db

@app.teardown_appcontext
def close_connection(exception):
    db = getattr(g, '_database', None)
    if db is not None:
        db.close()

@app.route('/')
def index():
    db = get_db()
    cursor = db.execute('SELECT SQLITE_VERSION()')
    data = cursor.fetchone()
    return f'SQLite version: {data[0]}'

if __name__ == '__main__':
    app.run(host='0.0.0.0')
```

**requirements.txt**

```python
Flask==2.0.2
```

### Step 2: Creating a Dockerfile with Multi-Stage Builds

Create a `Dockerfile` for your Flask application. Multi-stage builds help in creating smaller images by separating the build environment from the runtime environment.

**Dockerfile**

```dockerfile
# Stage 1: Build environment
FROM python:3.9-slim as builder

WORKDIR /app

# Install dependencies
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# Copy application code
COPY app.py .

# Stage 2: Runtime environment
FROM python:3.9-slim

WORKDIR /app

# Copy only the necessary files from the builder stage
COPY --from=builder /app /app

# Expose the port the app runs on
EXPOSE 5000

# Command to run the app
CMD ["python", "app.py"]
```

### Step 3: Building and Running the Docker Container

Build the Docker image:

```bash
docker build -t flask-sqlite-app .
```

Run the Docker container, mounting the SQLite database from the host filesystem:

```bash
docker run -d -p 5000:5000 -v $(pwd)/data:/data flask-sqlite-app
```

This command:

- Runs the container in detached mode (`-d`).
- Maps port 5000 of the container to port 5000 on the host (`-p 5000:5000`).
- Mounts the host directory `$(pwd)/data` to `/data` in the container (`-v $(pwd)/data:/data`).

### Step 4: Accessing the SQLite Database from the Host Filesystem

Ensure the `data` directory exists on your host:

```bash
mkdir -p data
```

You can now interact with the SQLite database directly from your host machine using tools like `sqlite3`:

```bash
sqlite3 data/database.db
```

### Step 5: Uploading the Docker Image to a Registry

To upload your Docker image to a registry like Docker Hub, follow these steps:

1. Tag your image:

   ```bash
   docker tag flask-sqlite-app yourusername/flask-sqlite-app:latest
   ```

2. Log in to Docker Hub:

   ```shell
   docker login
   ```

3. Push the image to Docker Hub:

   ```bash
   docker push yourusername/flask-sqlite-app:latest
   ```

### Tips for Efficient Docker Builds

1. **Use Multi-Stage Builds**: Separate build and runtime environments to reduce image size.
2. **Leverage Cache**: Organize Dockerfile instructions to maximize layer caching.
3. **Minimal Base Images**: Use lightweight base images like `python:3.9-slim` or `alpine`.
4. **Avoid Unnecessary Files**: Use `.dockerignore` to exclude files that are not needed in the image.

**.dockerignore**

```bash
__pycache__
*.pyc
*.pyo
*.pyd
```

### Accessing External Filesystems

In Docker, you can access external filesystems in several ways:

- **Bind Mounts**: Use `-v` to mount host directories into the container.
- **Volumes**: Use Docker-managed volumes for data persistence.
- **Network File Systems**: Mount NFS or other network shares into the container.

Example using a bind mount:

```bash
docker run -d -p 5000:5000 -v /path/on/host:/path/in/container flask-sqlite-app
```

### Conclusion

This tutorial covers the basics of setting up a Docker container for a Flask application with an SQLite database mounted from the host filesystem. It also includes tips for efficient Docker builds and uploading images to a registry. You can expand this setup based on your specific needs.

### Docker Volumes

Docker volumes are a preferred mechanism for persisting data generated by and used by Docker containers. Unlike bind mounts, which can be any directory on the host machine, volumes are completely managed by Docker and are stored in a part of the host filesystem that is managed by Docker (`/var/lib/docker/volumes/` on Linux).

#### Creating and Using a Volume

1. **Create a volume:**

   ```bash
   docker volume create my_volume
   ```

2. **Run a container with the volume:**

   ```bash
   docker run -d -p 5000:5000 -v my_volume:/data flask-sqlite-app
   ```

In this example, `my_volume` is a Docker-managed volume that is mounted to `/data` inside the container.

#### Inspecting a Volume

To inspect details about a volume, use:

```bash
docker volume inspect my_volume
```

### Tagging Images for Reproducibility

Tagging images correctly is crucial for ensuring reproducibility. Using `latest` as a tag can be convenient for development, but it can lead to issues in production where consistency is critical.

#### Example of Tagging

1. **Build an image with a specific version tag:**

   ```bash
   docker build -t flask-sqlite-app:v1.0.0 .
   ```

2. **Push the image to a registry with a version tag:**

   ```bash
   docker tag flask-sqlite-app:v1.0.0 yourusername/flask-sqlite-app:v1.0.0
   docker push yourusername/flask-sqlite-app:v1.0.0
   ```

By using version tags (`v1.0.0`), you ensure that the exact same image can be pulled and run, providing consistency across different environments and deployments.

#### Avoiding Mutating Tags

- **Semantic Versioning:** Use semantic versioning (e.g., `v1.0.0`, `v1.0.1`) to track changes and updates.
- **Git Commit Hashes:** Tag images with the Git commit hash (e.g., `yourusername/flask-sqlite-app:abcd1234`) to tie the image to a specific state of the codebase.

**Example: Tagging with a Git Commit Hash**

```bash
# Assuming you have a Git repository
COMMIT_HASH=$(git rev-parse --short HEAD)
docker build -t flask-sqlite-app:$COMMIT_HASH .
docker tag flask-sqlite-app:$COMMIT_HASH yourusername/flask-sqlite-app:$COMMIT_HASH
docker push yourusername/flask-sqlite-app:$COMMIT_HASH
```

### Summary

1. **Docker Volumes:**
   - Create and manage Docker volumes using `docker volume create`.
   - Use volumes in containers with the `-v` flag.
2. **Tagging for Reproducibility:**
   - Avoid using `latest` in production to ensure consistency.
   - Use semantic versioning or Git commit hashes for immutable tags.

By following these practices, you can manage persistent data effectively with Docker volumes and ensure that your Docker images are reproducible and consistent across different environments.



### Running Docker in Rootless Mode vs. Root Privileges

#### Docker with Root Privileges

Traditionally, Docker runs as a daemon (`dockerd`) with root privileges, which can pose security risks since it has extensive access to the host system.

**How it works:**

- The Docker daemon (`dockerd`) runs as the root user.
- Containers are managed by the daemon and run processes with root capabilities inside the container.

**Advantages:**

- Full functionality and compatibility with all Docker features.
- Simpler setup and management for users who require full Docker capabilities.

**Disadvantages:**

- Higher security risks due to the elevated privileges.
- Potential for privilege escalation attacks.

#### Docker Rootless Mode

Docker rootless mode allows running the Docker daemon and containers without requiring root privileges, enhancing security by limiting the impact of potential vulnerabilities.

**How it works:**

- The Docker daemon (`dockerd`) runs as a non-root user.
- User namespaces and other Linux kernel features are used to provide necessary container isolation.

**Advantages:**

- Enhanced security by running without root privileges.
- Reduces the attack surface by limiting access to host system resources.

**Disadvantages:**

- Limited functionality compared to running with root privileges.
- Some features and operations may not be supported.

**Setting up Docker in Rootless Mode:**

1. **Install Docker (Linux example):**

   ```
   bash
   Copy code
   curl -fsSL https://get.docker.com/rootless | sh
   ```

2. **Set up environment variables:**

   ```
   bash
   Copy code
   export PATH=/home/yourusername/bin:$PATH
   export DOCKER_HOST=unix:///run/user/$(id -u)/docker.sock
   ```

3. **Start Docker in rootless mode:**

   ```
   bash
   Copy code
   systemctl --user start docker
   ```

### Summary

- **Running Docker with Root Privileges:**
  - Traditional setup with full functionality but higher security risks.
- **Running Docker in Rootless Mode:**
  - Enhanced security with limited functionality. Use `curl -fsSL https://get.docker.com/rootless | sh`to install.



### Understanding RUN and ENTRYPOINT in Docker

#### RUN Command

- **Purpose:** The `RUN` command is used to execute commands in a new layer on top of the current image and commit the results. It is typically used for installing software packages and other setup tasks.

- **Usage:** `RUN` commands are executed during the build process of the Docker image.

- Example:

  ```dockerfile
  FROM ubuntu:20.04
  RUN apt-get update && apt-get install -y python3
  ```

#### ENTRYPOINT Command

- **Purpose:** The `ENTRYPOINT` command specifies the default command to run when a container starts. Unlike `RUN`, it is not executed during the build process but at runtime.

- **Usage:** `ENTRYPOINT` is used to define the main application that the container will run.

- Example:

  ```dockerfile
  FROM ubuntu:20.04
  ENTRYPOINT ["python3", "-m", "http.server"]
  ```

### Running Multiple Commands

#### Case 1: Multiple Commands in the Same Container

To run multiple commands in the same container, you can chain them together using shell operators (`&&`, `;`, etc.) in a single `RUN` or `CMD` instruction, or you can use a script that is executed by the `ENTRYPOINT` or `CMD`.

**Example Using a Script:**

1. **Create a script:**

   ```bash
   #!/bin/bash
   echo "Starting service 1..."
   service1 start
   echo "Starting service 2..."
   service2 start
   echo "All services started."
   tail -f /dev/null  # Keep the container running
   ```

2. **Dockerfile:**

   ```dockerfile
   FROM ubuntu:20.04
   
   # Copy the script into the container
   COPY start_services.sh /usr/local/bin/start_services.sh
   RUN chmod +x /usr/local/bin/start_services.sh
   
   # Set the ENTRYPOINT to the script
   ENTRYPOINT ["/usr/local/bin/start_services.sh"]
   ```

#### Case 2: Multiple Commands in Identical Instances of the Container

To run multiple stages of a pipeline in identical instances of the container, you can use a combination of `docker-compose`or Kubernetes to orchestrate the containers.

**Example Using Docker Compose:**

1. **Create a `docker-compose.yml` file:**

   ```yaml
   version: '3.8'
   
   services:
     stage1:
       image: my_pipeline_image
       command: ["bash", "-c", "echo Stage 1; sleep 10"]
   
     stage2:
       image: my_pipeline_image
       command: ["bash", "-c", "echo Stage 2; sleep 10"]
       depends_on:
         - stage1
   
     stage3:
       image: my_pipeline_image
       command: ["bash", "-c", "echo Stage 3; sleep 10"]
       depends_on:
         - stage2
   ```

2. **Run the pipeline:**

   ```bash
   docker-compose up
   ```

This setup ensures that each stage of the pipeline runs in its own container, but in the order defined by the `depends_on`configuration.

### Summary

- **RUN vs. ENTRYPOINT:**
  - `RUN`: Executes commands during the build process to create intermediate layers.
  - `ENTRYPOINT`: Specifies the default command to run at container start.
- **Running Multiple Commands:**
  - **In the same container:** Chain commands using shell operators or use a script with `ENTRYPOINT`.
  - **In identical instances of the container:** Use orchestration tools like Docker Compose to run multiple stages of a pipeline.



### How Docker Image Layers Work

1. **Layered Filesystem:**
   - Each Docker image consists of a series of read-only layers. These layers represent a filesystem state at different points in time.
   - When you build a Docker image, each instruction in your Dockerfile (like `FROM`, `RUN`, `COPY`, etc.) creates a new layer. These layers are stacked on top of each other to form the final image.
2. **Union Filesystem:**
   - Docker uses a union filesystem (like OverlayFS) to combine these layers into a single coherent filesystem. This allows Docker to present a unified view of the filesystem to the container.
3. **Caching:**
   - Docker caches each layer during the build process. If a layer hasn't changed, Docker can reuse the cached layer instead of rebuilding it, which speeds up the build process.
4. **Image Size:**
   - Layers are additive and immutable. Each layer includes only the changes from the previous layer.
   - Layers can be shared across multiple images. For example, if you have several images based on the same base image, they can share the same base layers, saving space.

### Writing Small and Effective Docker Images

#### Best Practices for Using Layers

1. **Minimize the Number of Layers:**

   - Combine multiple commands into a single `RUN` instruction to reduce the number of layers.

   - Example:

     ```dockerfile
     # Bad: Creates three layers
     RUN apt-get update
     RUN apt-get install -y package1
     RUN apt-get install -y package2
     
     # Good: Creates one layer
     RUN apt-get update && apt-get install -y package1 package2
     ```

2. **Order Matters:**

   - Place less frequently changing instructions (like installing OS packages) before more frequently changing instructions (like copying application code). This maximizes cache utilization.

   - Example:

     ```dockerfile
     # Good: Cache the result of installing dependencies
     COPY requirements.txt /app/
     RUN pip install --no-cache-dir -r /app/requirements.txt
     
     # Add the application code
     COPY . /app/
     ```

3. **Clean Up After Installations:**

   - Remove unnecessary files and clear package manager caches to reduce the size of the image.

   - Example:

     ```dockerfile
     RUN apt-get update && apt-get install -y \
         package1 \
         package2 \
         && rm -rf /var/lib/apt/lists/*
     ```

4. **Use Multi-Stage Builds:**

   - Multi-stage builds allow you to use multiple `FROM` statements in a single Dockerfile. You can use one stage to build the application and another to create the final image, copying only the necessary artifacts from the build stage.

   - Example:

     ```dockerfile
     # Stage 1: Build stage
     FROM golang:1.16 as builder
     WORKDIR /app
     COPY . .
     RUN go build -o myapp
     
     # Stage 2: Final image
     FROM alpine:latest
     WORKDIR /app
     COPY --from=builder /app/myapp .
     CMD ["./myapp"]
     ```

5. **Use a Small Base Image:**

   - Choose a minimal base image like `alpine` instead of larger ones like `ubuntu` or `debian`.

   - Example:

     ```dockerfile
     FROM alpine:latest
     RUN apk add --no-cache package1 package2
     ```

6. **Optimize COPY and ADD:**

   - Use `.dockerignore` to exclude unnecessary files from being copied into the image.

   - Example:

      

     .dockerignore:

     ```bash
     node_modules
     *.log
     ```

#### Consequences of Using Layers

1. **Efficiency:**
   - Reusing layers speeds up the build process and deployment time because Docker can cache and share layers.
   - Properly ordered and minimal layers reduce rebuild times when parts of the Dockerfile change.
2. **Image Size:**
   - Large or unnecessary layers can bloat the image size, leading to slower pulls and increased storage costs.
   - Cleaning up after installations and using multi-stage builds help keep the image size small.
3. **Security:**
   - Each layer is immutable, so vulnerabilities in any layer remain until the image is rebuilt. Keeping images up to date and minimizing layers helps maintain security.

### Conclusion

Using Docker image layers effectively involves understanding how they work, leveraging caching, and following best practices to minimize image size and maximize efficiency. By combining commands, using multi-stage builds, choosing small base images, and cleaning up after installations, you can create small, fast, and secure Docker images that make the most of Docker's capabilities.

